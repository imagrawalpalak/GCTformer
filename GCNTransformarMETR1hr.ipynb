{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1185b-6148-4dff-a8bf-4e9c38d1faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def load_data_from_h5(h5_path, dataset_key='df/block0_values'):\n",
    "    \"\"\"\n",
    "    Loads data from an HDF5 (.h5) file.\n",
    "    If dataset_key is None, loads the first dataset.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        # Split the dataset_key to handle nested groups\n",
    "        keys = dataset_key.split('/')\n",
    "        group = f\n",
    "        for key in keys[:-1]:\n",
    "            group = group.get(key)\n",
    "       \n",
    "        # Now, the last part of keys is the dataset name\n",
    "        dataset = group.get(keys[-1])\n",
    "\n",
    "        if isinstance(dataset, h5py.Dataset):\n",
    "            data = dataset[()]  # Safely load the dataset\n",
    "            print(f\"Loaded data shape: {data.shape}\")\n",
    "        else:\n",
    "            raise TypeError(f\"Dataset {dataset_key} is not of the expected type (h5py.Dataset), found: {type(dataset)}\")\n",
    "   \n",
    "    return data\n",
    "\n",
    "def fill_missing(data, method='zero'):\n",
    "    data = data.astype(float)\n",
    "    if method == 'zero':\n",
    "        return np.nan_to_num(data)\n",
    "    elif method == 'mean':\n",
    "        nan_mask = np.isnan(data)\n",
    "        col_mean = np.nanmean(data, axis=0)\n",
    "        data[nan_mask] = np.take(col_mean, np.where(nan_mask)[1])\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported fill method\")\n",
    "\n",
    "def normalize_data(data, scaler=None):\n",
    "    original_shape = data.shape\n",
    "    data_2d = data.reshape(-1, data.shape[-1])\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        data_2d = scaler.fit_transform(data_2d)\n",
    "    else:\n",
    "        data_2d = scaler.transform(data_2d)\n",
    "    return data_2d.reshape(original_shape), scaler\n",
    "\n",
    "def create_sliding_windows(data, window_size, horizon):\n",
    "    X, Y = [], []\n",
    "    for i in range(data.shape[0] - window_size - horizon + 1):\n",
    "        x_i = data[i:i+window_size]\n",
    "        y_i = data[i+window_size:i+window_size+horizon]\n",
    "        X.append(x_i)\n",
    "        Y.append(y_i)\n",
    "    X = np.stack(X)[:, :, :, np.newaxis]\n",
    "    Y = np.stack(Y)[:, :, :, np.newaxis]\n",
    "    return X, Y\n",
    "\n",
    "def main():\n",
    "    # Define the h5 file path\n",
    "    h5_file = os.path.expanduser('~/Desktop/TrafficPrediction/Enhanced/data/METR.h5')\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(h5_file):\n",
    "        raise FileNotFoundError(f\"The file {h5_file} was not found.\")\n",
    "\n",
    "    # Load data with the correct dataset key\n",
    "    data = load_data_from_h5(h5_file, dataset_key='df/block0_values')  # Correct dataset_key\n",
    "\n",
    "    # Handle missing values by filling with the mean\n",
    "    data = fill_missing(data, method='mean')\n",
    "\n",
    "    # Normalize data to [0, 1] range\n",
    "    data, scaler = normalize_data(data)\n",
    "\n",
    "    # Create sliding windows for training\n",
    "    window_size = 12  # Past 1 hour of data (assuming 5-minute intervals, 12 * 5 = 60 minutes)\n",
    "    horizon = 12      # Predict the next 1 hour (12 * 5 = 60 minutes)\n",
    "    X, Y = create_sliding_windows(data, window_size, horizon)\n",
    "\n",
    "    print(f\"X shape: {X.shape} (samples, time_steps, nodes, features)\")\n",
    "    print(f\"Y shape: {Y.shape} (prediction targets)\")\n",
    "\n",
    "    # Save the preprocessed data as NumPy files for use in the Dataset class\n",
    "    np.save('X.npy', X)\n",
    "    np.save('Y.npy', Y)\n",
    "    print(\"Preprocessed data saved as X.npy and Y.npy\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193b9fc-52ae-490c-a341-d358d94f248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, x_path, y_path):\n",
    "        # Load the preprocessed data\n",
    "        self.X = np.load(x_path)  # shape: (samples, time_steps, nodes, 1)\n",
    "        self.Y = np.load(y_path)  # shape: (samples, horizon, nodes, 1)\n",
    "\n",
    "        # Convert to torch tensors and permute to [samples, features, time, nodes]\n",
    "        self.X = torch.from_numpy(self.X).float().permute(0, 3, 1, 2)  # → [N, 1, 12, 500]\n",
    "        self.Y = torch.from_numpy(self.Y).float().permute(0, 3, 1, 2)  # → [N, 1, 3, 500]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcfa2f-bd64-46e3-8320-3704c5c3bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Load dataset\n",
    "dataset = TrafficDataset('X.npy', 'Y.npy')\n",
    "\n",
    "# Split into train, val, test\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0239e88-4403-4249-b9b2-3057b7faf8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, adj_matrix, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.adj_matrix = adj_matrix  # Adjacency matrix with shape [N, N]\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # Learnable transformation matrix for node features\n",
    "        self.theta = nn.Parameter(torch.randn(in_channels, out_channels))  # [F_in, F_out]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [B, T, N, F_in]\n",
    "        adj_matrix shape: [N, N] (with N nodes)\n",
    "        \"\"\"\n",
    "        B, T, N, F_in = x.shape\n",
    "        \n",
    "        # Expand adjacency matrix to match the batch and time dimensions\n",
    "        adj_matrix_expanded = self.adj_matrix.unsqueeze(0).unsqueeze(0).expand(B, T, N, N)  # [B, T, N, N]\n",
    "\n",
    "        # Propagate node features based on the adjacency matrix\n",
    "        # Apply adjacency matrix to propagate information between nodes for each time step and batch\n",
    "        x = torch.einsum('btnf,btnn->btnf', x, adj_matrix_expanded)  # [B, T, N, F_in] x [B, T, N, N] -> [B, T, N, F_in]\n",
    "        \n",
    "        # Apply the learned transformation matrix (theta) to the node features\n",
    "        x = torch.einsum('btnf,fo->btno', x, self.theta)  # [B, T, N, F_in] x [F_in, F_out] -> [B, T, N, F_out]\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cd83c-d764-48b8-9a1b-a20d137a4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, nhead, num_layers, dim_feedforward=512):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=input_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, T, N, F_in]\n",
    "        B, T, N, F_in = x.shape\n",
    "\n",
    "        # Reshape x to fit transformer input shape [B * N, T, F_in]\n",
    "        x = x.view(B * N, T, F_in)\n",
    "\n",
    "        # Pass through the transformer\n",
    "        x = self.transformer(x, x)  # Self-attention on both source and target\n",
    "\n",
    "        # Reshape back to [B, T, N, F_out]\n",
    "        x = x.view(B, T, N, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd013f1-4f4f-44f0-bca4-c29050a7bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalModel(nn.Module):\n",
    "    def __init__(self, adj_matrix, gcn_in_channels, gcn_out_channels, transformer_input_dim, nhead, num_layers, num_classes):\n",
    "        super(SpatioTemporalModel, self).__init__()\n",
    "        \n",
    "        # Initialize the GCN layer with the correct order of arguments\n",
    "        self.gcn = GCNLayer(adj_matrix, gcn_in_channels, gcn_out_channels)  # Corrected argument order\n",
    "        \n",
    "        # Initialize Transformer layer\n",
    "        self.transformer = TransformerLayer(transformer_input_dim, nhead, num_layers)\n",
    "        \n",
    "        # Fully connected layer for output prediction\n",
    "        self.fc = nn.Linear(transformer_input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gcn(x)  # Apply GCN layer\n",
    "        x = self.transformer(x)  # Apply Transformer layer\n",
    "        x = x[:, -1, :, :]  # Use the last time step for prediction\n",
    "        x = self.fc(x)  # Final prediction layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e7041-3115-4dae-9e87-6b496c846e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=100, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(X_batch)\n",
    "            loss = criterion(Y_pred, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch in val_loader:\n",
    "                Y_pred = model(X_batch)\n",
    "                loss = criterion(Y_pred, Y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "def calculate_metrics(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in data_loader:\n",
    "            Y_pred = model(X_batch)\n",
    "            y_true.append(Y_batch.cpu().numpy())\n",
    "            y_pred.append(Y_pred.cpu().numpy())\n",
    "   \n",
    "    y_true = np.concatenate(y_true).flatten()\n",
    "    y_pred = np.concatenate(y_pred).flatten()\n",
    "   \n",
    "    # Calculate MAE (Mean Absolute Error)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "   \n",
    "    # Calculate RMSE (Root Mean Squared Error)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "   \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    # Handle division by zero by masking non-zero true values\n",
    "    nonzero_mask = y_true != 0\n",
    "    if np.any(nonzero_mask):\n",
    "        mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 10\n",
    "    else:\n",
    "        mape = float('nan')  # MAPE undefined if all true values are zero\n",
    "   \n",
    "    print(\"\\n=== Evaluation Metrics ===\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}%\")  # Display as percentage\n",
    "   \n",
    "    return mae, rmse, mape\n",
    "\n",
    "def calculate_accuracy(model, data_loader, tolerance=0.05):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in data_loader:\n",
    "            Y_pred = model(X_batch)\n",
    "            Y_pred_flat = Y_pred.view(-1)\n",
    "            Y_batch_flat = Y_batch.view(-1)\n",
    "            correct += ((torch.abs(Y_pred_flat - Y_batch_flat) < tolerance).sum().item())\n",
    "            total += Y_batch_flat.numel()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy (±{tolerance} tolerance): {accuracy:.4f}\")\n",
    "\n",
    "# Dummy setup (replace with real data and model)\n",
    "N = 500  # Nodes\n",
    "B, T, F_in = 32, 10, 16\n",
    "adj_matrix = torch.rand(N, N)\n",
    "\n",
    "X_train = torch.randn(B, T, N, F_in)\n",
    "Y_train = torch.randn(B, N, 1)\n",
    "\n",
    "train_data = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=B, shuffle=True)\n",
    "\n",
    "# Replace this with your actual SpatioTemporalModel definition\n",
    "model = SpatioTemporalModel(adj_matrix=adj_matrix, gcn_in_channels=F_in, gcn_out_channels=64,\n",
    "                            transformer_input_dim=64, nhead=4, num_layers=2, num_classes=1)\n",
    "\n",
    "# Train and evaluate\n",
    "train_model(model, train_loader, train_loader, epochs=100)\n",
    "calculate_metrics(model, train_loader)\n",
    "calculate_accuracy(model, train_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
